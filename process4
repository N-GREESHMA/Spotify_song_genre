@@ -30,16 +30,27 @@ access_token <- get_spotify_access_token(client_id = Sys.getenv('SPOTIFY_CLIENT_
                                         client_secret = Sys.getenv('SPOTIFY_CLIENT_SECRET'))

# Get a list of genre-specific playlists
genres <- c('pop', 'country', 'rap', 'hiphop', 'rock', 'dance')
genres <- c('pop', 'r&b', 'rap', 'latin', 'rock', 'edm')
### every noise
# http://everynoise.com/everynoise1d.cgi?root=edm
subgenres <- data.frame(genre = c(rep('pop',4), rep('rap',4), rep('rock',4), rep('latin',4), rep('r&b',4), rep('edm',4)),
                      subgenre = c('dance pop', 'post-teen pop', 'electropop', 'indie poptimism', 
                                    'hip hop', 'southern hip hop', 'gangster rap', 'trap', 
                                    'album rock', 'classic rock', 'permanent wave', 'hard rock',
                                    'tropical', 'latin pop', 'reggaeton', 'latin hip hop', 
                                    'urban contemporary', 'hip pop', 'new jack swing', 'neo soul',
                                    'electro house', 'big room', 'pop edm', 'progressive electro house'),
                      stringsAsFactors = FALSE)

playlist_ids <- NULL

for(g in genres){
for(g in seq_along(subgenres$subgenre)){

  out <- search_spotify(q = g, type = 'playlist')
  out <- search_spotify(q = subgenres$subgenre[g], type = 'playlist', market = 'US', limit = 20)
  out <- out %>% 
    select(name, id) %>%
    mutate(genre = g)
    mutate(subgenre = subgenres$subgenre[g],
           genre = subgenres$genre[g])

  playlist_ids <- rbind(playlist_ids, out)

@@ -64,7 +75,8 @@ for(p in seq_along(playlist_ids$id)){
    select(track.id, track.name, track.artist, track.popularity, track.album.id, track.album.name, track.album.release_date) %>%
    mutate(playlist_name = playlist_ids$name[p],
           playlist_id = playlist_ids$id[p],
           playlist_genre = playlist_ids$genre[p]) 
           playlist_genre = playlist_ids$genre[p],
           playlist_subgenre = playlist_ids$subgenre[p]) 

  playlist_songs <- rbind(playlist_songs, out)

@@ -76,6 +88,15 @@ playlist_audio <- get_track_audio_features_over_100(ids = playlist_songs$track.i
# combine
playlist_songs <- playlist_songs %>%
  left_join(select(playlist_audio, -track_href, -uri, -analysis_url, -type, -time_signature), by = c('track.id' = 'id')) %>%
  unique()
  unique() %>%
  filter(!is.na(danceability))

#write.csv(playlist_songs, 'genre_songs.csv', row.names=FALSE)
# handle duplicates - songs on multiple playlists
playlist_songs <- playlist_songs %>% 
  group_by(playlist_genre, playlist_subgenre, track.id) %>%
  mutate(row_number = 1:n()) %>%
  filter(row_number == 1) %>%
  ungroup() %>%
  select(-row_number)

#write.csv(playlist_songs, 'genre_songs.csv', row.names=FALSE)
252 changes: 204 additions & 48 deletions 252
spotify_genres.Rmd
@@ -6,37 +6,22 @@ output: github_document
Classifying songs into major music genres
Understanding the how Spotify's audio features map onto major music genres  

Spotify provies 12 [audio features](https://developer.spotify.com/documentation/web-api/reference/object-model/#audio-features-object) for each track (paraphrased below):    

- **acousticness**: confidence measure of whether or not the track is acoustic (0.0-1.0)  
- **danceability**: how suitable a track is for dancing, based on a combination of elements like tempo, beat strength, and overall regularity (0.0-1.0)  
- **duration_ms**: the duration of the track in milliseconds.  
- **energy**: a perceptual measure of intensity and activity, based on features like dynamic range, perceived loudness, timbre and onset rate (0.0-1.0)    
- **instrumentalness**: confidence measure of whether or not a track is instrumental (no vocals) or not (0.0-1.0)  
- **key**: the key of the track, integers mapping to pitches    
- **liveness**: confidence measure of whether or not the track was recorded live/contains audience noises (0.0-1.0)  
- **loudness**: overall loudness of a track in decibels (typical range -60 to 0 db)   
- **mode**: major (1) or minor (0) modality of the track  
- **speechiness**: detects the presence of spoken words, where 0-0.33 is most likely music without speech, 0.33-0.66 contain both music and speech, and 0.66-1.0 are most likely speech without music (0.0-1.0)  
- **tempo**: the estimated speed or pace of the track in beats per minute (bpm)  
- **valence**: describes the positiveness conveyed by a track (0.0-1.0)  


```{r setup, include=FALSE}
Spotify provies 12 [audio features](https://developer.spotify.com/documentation/web-api/reference/object-model/#audio-features-object) for each track, including confidence measures like `acousticness`, `liveness`, `speechiness` and `instrumentalness`, perceptual measures like `energy`, `loudness`, `danceability` and `valuence` (positiveness), and descriptors like `duration`, `tempo`, `key`, and `mode`.  

```{r setup, warning = FALSE, error = FALSE, message= FALSE}
library(tidyverse)
library(randomForest)
library(formattable)
source('../kp_themes/theme_kp.R')
knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 6)
```

```{r inspect, fig.height = 10, fig.width = 12}
# refer to spotify_dataset.R for how this dataset was generated
playlist_songs <- read.csv('genre_songs.csv', stringsAsFactors = FALSE) %>%
  filter(!is.na(danceability))
```
playlist_songs <- read.csv('genre_songs_expanded.csv', stringsAsFactors = FALSE) 
```{r inspect, fig.width = 10, fig.height = 8}
feature_names <- names(playlist_songs)[11:22]
feature_names <- names(playlist_songs)[12:23]
playlist_songs %>%
  select(c('playlist_genre', feature_names)) %>%
@@ -47,12 +32,15 @@ playlist_songs %>%
  labs(title = 'Spotify Audio Feature Density - by Genre',
       x = '', y = 'density') +
  theme_kp() +
  theme(axis.text.y = element_blank()) + 
  scale_color_kp(palette = 'mixed')
```

Overall, the songs in the dataset tend to have low acousticness, liveness, instrumentalness and speechiness, with higher danceability, energy, loudness, and tempos. Valence is evenly distributed.  
Overall, the songs in the dataset tend to have low acousticness, liveness, instrumentalness and speechiness, with higher danceability, energy, and loudness. Valence is evenly distributed.  

Breaking things out by genre, EDM tracks are most likely to not be acoustic and have high energy with low valence (sad or depressed); latin tracks have high valence (are positive or cheerful) and danceability; rap songs score highly for speechiness and danceability; and rock songs are most likely to be recorded live and have low danceability.   

Breaking things out by genre, dance tracks are most likely to not be acoustic and to have high energy, while rap and hiphop tend to score highest for danceability. Based on the density plot, it looks like energy and danceability may provide the most separation between genres during classification, while instrumentalness and key may not help much.  
Based on the density plot, it looks like energy, valence, and danceability may provide the most separation between genres during classification, while instrumentalness and key and key may not help much.  

How do these features correlate with one another? Are there any that are redundant?  

@@ -69,37 +57,44 @@ playlist_songs %>%
                     addCoef.col = "grey30",
                     number.cex = 0.5,
                     col = colorRampPalette(colors = c(kp_cols('red'), 'white', kp_cols('dark_blue')))(200),
                     main = '\nAudio Feature Correlation',
                     main = 'Audio Feature Correlation',
                     mar = c(2,2,2,2),
                     family = 'Avenir')
```

Energy and loudness appear to be collinear (as they have a correlation of 0.73). Let's remove loudness, since energy appears to give more distinction between genre groups (as seen in the density plot).  
Energy and loudness are fairly highly correlated (0.68). Let's remove loudness, since energy appears to give more distinction between genre groups (as seen in the density plot).  

```{r}
# remove loudness
feature_names <- names(playlist_songs)[c(11:13,15:22)]
feature_names_reduced <- names(playlist_songs)[c(12:14,16:23)]
```

Key doesn't appear to have associations, positive or negative, with any of the other features aside from mode. 

How do the genres correlate with each other? How consistent are songs within a given genre?  

```{r cor, fig.height = 5, fig.width = 4}
```{r genre_cor, fig.height = 5, fig.width = 5}
# take a random sample
set.seed(0012)
song_sample <- sample(1:nrow(playlist_songs), nrow(playlist_songs)*.05, replace = FALSE)
# create a key dataframe with ids/genres with numerical index
key <- playlist_songs %>%
  select(track.id, playlist_genre) %>%
  mutate(position = 1:n())
  mutate(position = as.character(1:n()))
key <- key[song_sample, ]
# create a correlation matrix, then melt it
song_cor <- playlist_songs %>% 
  select(feature_names) %>%
  scale(center = TRUE) %>%
song_cor <- playlist_songs[song_sample, ] %>% 
  select(feature_names_reduced) %>%
  scale() %>%
  t() %>%
  cor() %>%
  reshape2::melt() %>%
  filter(!is.na(value) & Var1 != Var2) %>%
  left_join(key, by = c('Var1' = 'position')) %>%
  left_join(key, by = c('Var2' = 'position')) 
  as.data.frame() %>%
  mutate(index = row.names(.)) %>%
  pivot_longer(-index) %>%
  filter(!is.na(value) & index != name) %>%
  left_join(key, by = c('index' = 'position')) %>%
  left_join(key, by = c('name' = 'position')) 
# summarise by genres
genre_cor <- song_cor %>%
@@ -108,7 +103,9 @@ genre_cor <- song_cor %>%
  ungroup() 
genre_cor_matrix <- genre_cor %>%
  reshape2::dcast(playlist_genre.x ~ playlist_genre.y, value.var = 'avg_cor') 
  pivot_wider(id_cols = 'playlist_genre.x', 
              names_from = 'playlist_genre.y', 
              values_from = 'avg_cor')
row.names(genre_cor_matrix) <- genre_cor_matrix$playlist_genre.x
@@ -122,24 +119,23 @@ genre_cor_matrix %>%
                     addCoef.col = "grey40",
                     number.cex = 0.75,
                     col = colorRampPalette(colors = c(kp_cols('red'), 'white', kp_cols('dark_blue')))(200),
                     mar = c(2,2,2,2),
                     main = '\nAverage Correlation Between Genre Songs',
                     family = 'Avenir'
                     )
```

Songs within each genre vary quite a bit! Country songs are the most consistent, with a correlation strength of 0.17, while pop songs are the least consistent at just 0.03.  
Songs within each genre vary quite a bit! EDM and rock songs are the most consistent, with a correlation strength of 0.12, while pop songs are the least consistent at just 0.02.  

Rap and hiphop (0.11) and country and rock (0.10) have the strongest correlation across genres. They also have the strongest negative association with one another (-0.12 - -0.11). Dance and pop have very little positive or negative associations across genres, which may make them hard to classify.  
R&B and EDM (-0.08) and rap and rock(-0.08) have the strongest negative correlations of any genre pairs.The rest of the genres don't negatively or positively correlate much with one another, which may make them hard to classify.    

### Preparing the data for training  

```{r split}
# split into testing and training
set.seed(1234)
training_songs <- sample(1:nrow(playlist_songs), nrow(playlist_songs)*.80, replace = FALSE)
train_set <- playlist_songs[training_songs, c('playlist_genre', feature_names)] 
test_set <- playlist_songs[-training_songs, c('playlist_genre', feature_names)] 
train_set <- playlist_songs[training_songs, c('playlist_genre', feature_names_reduced)] 
test_set <- playlist_songs[-training_songs, c('playlist_genre', feature_names_reduced)] 
train_resp <- playlist_songs[training_songs, 'playlist_genre']
test_resp <- playlist_songs[-training_songs, 'playlist_genre']
@@ -149,6 +145,32 @@ test_resp <- playlist_songs[-training_songs, 'playlist_genre']

```{r knn}
# run k nearest neighbors 
# at various values of k
select_k <- NULL
for(i in 1:10){
  kresult <- class::knn(train = train_set[,-1], test = test_set[,-1], cl = train_resp, k = i)
  compare_knn <- data.frame(true_value = test_resp,
                                predicted_value = kresult,
                                stringsAsFactors = FALSE) %>%
    count(true_value, predicted_value) %>%
    mutate(match = ifelse(true_value == predicted_value, TRUE, FALSE))
  
  accuracy_knn <- compare_knn %>%
    group_by(match) %>%
    summarise(n = sum(n)) %>%
    ungroup() %>%
    mutate(percent = n/sum(n),
           k = i,
           model = 'knn') %>%
    filter(match == TRUE)
    
  select_k <- rbind(select_k, accuracy_knn)
  
}
# 1 is the best
kresult <- class::knn(train = train_set[,-1], test = test_set[,-1], cl = train_resp, k = 1)
# check
@@ -182,9 +204,17 @@ compare_knn %>%


```{r random_forest}
rfresult <- randomForest(as.factor(playlist_genre) ~ ., ntree = 100, data = train_set)
varImpPlot(rfresult, sort = TRUE, n.var = 11)
rfresult <- randomForest(as.factor(playlist_genre) ~ ., ntree = 100, importance = TRUE, data = train_set)
importance(rfresult, type = 1) %>%
  as.data.frame() %>%
  mutate(measure = row.names(.)) %>%
  ggplot(aes(x = reorder(measure, MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) +
  geom_point() +
  coord_flip() +
  theme_kp() +
  labs(title = 'Random Forest Variable Importance',
       y = 'Mean Decrease in Accuracy', x = '')
predict_rf <- predict(rfresult, test_set)
@@ -222,4 +252,130 @@ accuracy_knn %>%
  knitr::kable()
```

### Appendix  

```{r subgenres, include = FALSE, fig.height = 15, fig.width = 12}
# subgenre_density_plot <- function(genre_name){
#   
#   playlist_songs %>%
#     filter(playlist_genre == genre_name) %>%
#     select(c('playlist_subgenre', feature_names)) %>%
#     pivot_longer(cols = feature_names) %>%
#     ggplot(aes(x = value)) +
#     geom_density(aes(color = playlist_subgenre), alpha = 0.5) +
#     facet_wrap(~name, ncol = 3, scales = 'free') +
#     labs(title = paste0('Subgenre Audio Feature Density (', str_to_title(genre_name), ')'),
#          x = '', y = 'density') +
#     theme_kp() +
#     theme(axis.text.y = element_blank()) + 
#     scale_color_kp(palette = 'mixed')
#   
# }
# 
# gridExtra::grid.arrange(
#   subgenre_density_plot('rap'),
#   subgenre_density_plot('pop'),
#   subgenre_density_plot('r&b'),
#   subgenre_density_plot('latin'),
#   subgenre_density_plot('edm'),
#   subgenre_density_plot('rock'),
#   ncol = 2)
```


```{r include = FALSE}
# genres <- playlist_songs$playlist_genre %>% unique()
# 
# subgenre_rf <- NULL
# subgenre_importance <- NULL
# subgenre_plots <- list()
# 
# for(g in seq_along(genres)){
#   
#   song_temp <- playlist_songs %>% 
#     filter(playlist_genre == genres[g] & !is.na(danceability))
#   
#   set.seed(1234)
#   training_songs_temp <- sample(1:nrow(song_temp), nrow(song_temp)*.80, replace = FALSE)
#   train_set_temp <- song_temp[training_songs_temp, c('playlist_subgenre', feature_names_reduced)] 
#   test_set_temp <- song_temp[-training_songs_temp, c('playlist_subgenre', feature_names_reduced)] 
#   
#   train_resp_temp <- song_temp[training_songs_temp, 'playlist_subgenre']
#   test_resp_temp <- song_temp[-training_songs_temp, 'playlist_subgenre']
#   
#   result_temp <- randomForest(as.factor(playlist_subgenre) ~ ., ntree = 100, importance = TRUE, data = train_set_temp)
#   
#   # save importance
#   importance_temp <- importance(result_temp, type = 1) %>% 
#     as.data.frame() %>%
#     mutate(measure = row.names(.),
#            genre = genres[g])
#   
#   subgenre_importance <- rbind(subgenre_importance, importance_temp)
#   
#   ## save prediction plot
#   predict_temp <- predict(result_temp, test_set_temp)
# 
#   compare_temp <- test_set_temp %>%
#     cbind(predict_temp) %>%
#     count(playlist_subgenre, predict_temp) %>%
#     mutate(match = ifelse(playlist_subgenre == predict_temp, TRUE, FALSE))
#   
#   subgenre_plots[[g]] <- compare_temp %>%
#     ggplot(aes(x = playlist_subgenre, y = n)) +
#     geom_col(aes(fill = match), position = 'dodge') +
#     facet_wrap(~predict_temp, ncol = 2) +
#     coord_flip() + 
#     labs(title = 'Random Forest Accuracy',
#          subtitle = genres[g],
#          x = '', y = 'count') +
#     theme_kp() +
#     scale_fill_kp()
#   
#   ## save overall accuracy
#   accuracy_temp <- compare_temp %>%
#     group_by(match) %>%
#     summarise(n = sum(n)) %>%
#     ungroup() %>%
#     mutate(percent = n/sum(n),
#            model = genres[g]) %>%
#     filter(match == TRUE)
#   
#   subgenre_rf <- rbind(subgenre_rf, accuracy_temp)
#   
# }
```

```{r subgenre_importance, include = FALSE}
# subgenre_importance %>%
#   ggplot(aes(x = reorder(measure, MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) +
#   geom_point(aes(color = genre)) +
#   facet_wrap(~genre) +
#   coord_flip() +
#   theme_kp() +
#   scale_color_kp() +
#   labs(title = 'Variable Importance', x = '')
```

```{r subgenre_acccuracy, include = FALSE}
# subgenre_rf %>% 
#   select(model, percent) %>%
#   mutate(percent = percent(percent,2)) %>%
#   knitr::kable()
```

```{r subgenre_plots, include = FALSE, fig.height = 15, fig.width = 12}
# gridExtra::grid.arrange(
#   subgenre_plots[[1]],
#   subgenre_plots[[2]],
#   subgenre_plots[[3]],
#   subgenre_plots[[4]],
#   subgenre_plots[[5]],
#   subgenre_plots[[6]],
#   ncol = 2
#   )
```


184 changes: 114 additions & 70 deletions 184
spotify_genres.md
@@ -6,38 +6,25 @@ Spotify’s audio features map onto major music genres

Spotify provies 12 [audio
features](https://developer.spotify.com/documentation/web-api/reference/object-model/#audio-features-object)
for each track (paraphrased below):

  - **acousticness**: confidence measure of whether or not the track is
    acoustic (0.0-1.0)  
  - **danceability**: how suitable a track is for dancing, based on a
    combination of elements like tempo, beat strength, and overall
    regularity (0.0-1.0)  
  - **duration\_ms**: the duration of the track in milliseconds.  
  - **energy**: a perceptual measure of intensity and activity, based on
    features like dynamic range, perceived loudness, timbre and onset
    rate (0.0-1.0)  
  - **instrumentalness**: confidence measure of whether or not a track
    is instrumental (no vocals) or not (0.0-1.0)  
  - **key**: the key of the track, integers mapping to pitches  
  - **liveness**: confidence measure of whether or not the track was
    recorded live/contains audience noises (0.0-1.0)  
  - **loudness**: overall loudness of a track in decibels (typical range
    -60 to 0 db)  
  - **mode**: major (1) or minor (0) modality of the track  
  - **speechiness**: detects the presence of spoken words, where 0-0.33
    is most likely music without speech, 0.33-0.66 contain both music
    and speech, and 0.66-1.0 are most likely speech without music
    (0.0-1.0)  
  - **tempo**: the estimated speed or pace of the track in beats per
    minute (bpm)  
  - **valence**: describes the positiveness conveyed by a track
    (0.0-1.0)

<!-- end list -->
for each track, including confidence measures like `acousticness`,
`liveness`, `speechiness` and `instrumentalness`, perceptual measures
like `energy`, `loudness`, `danceability` and `valuence` (positiveness),
and descriptors like `duration`, `tempo`, `key`, and `mode`.

``` r
feature_names <- names(playlist_songs)[11:22]
library(tidyverse)
library(randomForest)
library(formattable)
source('../kp_themes/theme_kp.R')

knitr::opts_chunk$set(echo = TRUE, fig.width = 8, fig.height = 6)
```

``` r
# refer to spotify_dataset.R for how this dataset was generated
playlist_songs <- read.csv('genre_songs_expanded.csv', stringsAsFactors = FALSE) 

feature_names <- names(playlist_songs)[12:23]

playlist_songs %>%
  select(c('playlist_genre', feature_names)) %>%
@@ -48,20 +35,26 @@ playlist_songs %>%
  labs(title = 'Spotify Audio Feature Density - by Genre',
       x = '', y = 'density') +
  theme_kp() +
  theme(axis.text.y = element_blank()) + 
  scale_color_kp(palette = 'mixed')
```

![](spotify_genres_files/figure-gfm/inspect-1.png)<!-- -->

Overall, the songs in the dataset tend to have low acousticness,
liveness, instrumentalness and speechiness, with higher danceability,
energy, loudness, and tempos. Valence is evenly distributed.
energy, and loudness. Valence is evenly distributed.

Breaking things out by genre, dance tracks are most likely to not be
acoustic and to have high energy, while rap and hiphop tend to score
highest for danceability. Based on the density plot, it looks like
energy and danceability may provide the most separation between genres
during classification, while instrumentalness and key may not help much.
Breaking things out by genre, EDM tracks are most likely to not be
acoustic and have high energy with low valence (sad or depressed); latin
tracks have high valence (are positive or cheerful) and danceability;
rap songs score highly for speechiness and danceability; and rock songs
are most likely to be recorded live and have low danceability.

Based on the density plot, it looks like energy, valence, and
danceability may provide the most separation between genres during
classification, while instrumentalness and key and key may not help
much.

How do these features correlate with one another? Are there any that are
redundant?
@@ -79,43 +72,49 @@ playlist_songs %>%
                     addCoef.col = "grey30",
                     number.cex = 0.5,
                     col = colorRampPalette(colors = c(kp_cols('red'), 'white', kp_cols('dark_blue')))(200),
                     main = '\nAudio Feature Correlation',
                     main = 'Audio Feature Correlation',
                     mar = c(2,2,2,2),
                     family = 'Avenir')
```

![](spotify_genres_files/figure-gfm/explore_features-1.png)<!-- -->

Energy and loudness appear to be collinear (as they have a correlation
of 0.73). Let’s remove loudness, since energy appears to give more
distinction between genre groups (as seen in the density plot).
Energy and loudness are fairly highly correlated (0.68). Let’s remove
loudness, since energy appears to give more distinction between genre
groups (as seen in the density plot).

``` r
# remove loudness
feature_names <- names(playlist_songs)[c(11:13,15:22)]
feature_names_reduced <- names(playlist_songs)[c(12:14,16:23)]
```

Key doesn’t appear to have associations, positive or negative, with any
of the other features aside from mode.

How do the genres correlate with each other? How consistent are songs
within a given genre?

``` r
# take a random sample
set.seed(0012)
song_sample <- sample(1:nrow(playlist_songs), nrow(playlist_songs)*.05, replace = FALSE)

# create a key dataframe with ids/genres with numerical index
key <- playlist_songs %>%
  select(track.id, playlist_genre) %>%
  mutate(position = 1:n())
  mutate(position = as.character(1:n()))

key <- key[song_sample, ]

# create a correlation matrix, then melt it
song_cor <- playlist_songs %>% 
  select(feature_names) %>%
  scale(center = TRUE) %>%
song_cor <- playlist_songs[song_sample, ] %>% 
  select(feature_names_reduced) %>%
  scale() %>%
  t() %>%
  cor() %>%
  reshape2::melt() %>%
  filter(!is.na(value) & Var1 != Var2) %>%
  left_join(key, by = c('Var1' = 'position')) %>%
  left_join(key, by = c('Var2' = 'position')) 
  as.data.frame() %>%
  mutate(index = row.names(.)) %>%
  pivot_longer(-index) %>%
  filter(!is.na(value) & index != name) %>%
  left_join(key, by = c('index' = 'position')) %>%
  left_join(key, by = c('name' = 'position')) 

# summarise by genres
genre_cor <- song_cor %>%
@@ -124,10 +123,16 @@ genre_cor <- song_cor %>%
  ungroup() 

genre_cor_matrix <- genre_cor %>%
  reshape2::dcast(playlist_genre.x ~ playlist_genre.y, value.var = 'avg_cor') 
  pivot_wider(id_cols = 'playlist_genre.x', 
              names_from = 'playlist_genre.y', 
              values_from = 'avg_cor')

row.names(genre_cor_matrix) <- genre_cor_matrix$playlist_genre.x
```

    ## Warning: Setting row names on a tibble is deprecated.

``` r
genre_cor_matrix %>%
  select(-playlist_genre.x) %>%
  as.matrix() %>%
@@ -138,31 +143,30 @@ genre_cor_matrix %>%
                     addCoef.col = "grey40",
                     number.cex = 0.75,
                     col = colorRampPalette(colors = c(kp_cols('red'), 'white', kp_cols('dark_blue')))(200),
                     mar = c(2,2,2,2),
                     main = '\nAverage Correlation Between Genre Songs',
                     family = 'Avenir'
                     )
```

![](spotify_genres_files/figure-gfm/cor-1.png)<!-- -->
![](spotify_genres_files/figure-gfm/genre_cor-1.png)<!-- -->

Songs within each genre vary quite a bit\! Country songs are the most
consistent, with a correlation strength of 0.17, while pop songs are the
least consistent at just 0.03.
Songs within each genre vary quite a bit\! EDM and rock songs are the
most consistent, with a correlation strength of 0.12, while pop songs
are the least consistent at just 0.02.

Rap and hiphop (0.11) and country and rock (0.10) have the strongest
correlation across genres. They also have the strongest negative
association with one another (-0.12 - -0.11). Dance and pop have very
little positive or negative associations across genres, which may make
them hard to classify.
R\&B and EDM (-0.08) and rap and rock(-0.08) have the strongest negative
correlations of any genre pairs.The rest of the genres don’t negatively
or positively correlate much with one another, which may make them hard
to classify.

### Preparing the data for training

``` r
# split into testing and training
set.seed(1234)
training_songs <- sample(1:nrow(playlist_songs), nrow(playlist_songs)*.80, replace = FALSE)
train_set <- playlist_songs[training_songs, c('playlist_genre', feature_names)] 
test_set <- playlist_songs[-training_songs, c('playlist_genre', feature_names)] 
train_set <- playlist_songs[training_songs, c('playlist_genre', feature_names_reduced)] 
test_set <- playlist_songs[-training_songs, c('playlist_genre', feature_names_reduced)] 

train_resp <- playlist_songs[training_songs, 'playlist_genre']
test_resp <- playlist_songs[-training_songs, 'playlist_genre']
@@ -172,6 +176,32 @@ test_resp <- playlist_songs[-training_songs, 'playlist_genre']

``` r
# run k nearest neighbors 
# at various values of k
select_k <- NULL

for(i in 1:10){
  kresult <- class::knn(train = train_set[,-1], test = test_set[,-1], cl = train_resp, k = i)

  compare_knn <- data.frame(true_value = test_resp,
                                predicted_value = kresult,
                                stringsAsFactors = FALSE) %>%
    count(true_value, predicted_value) %>%
    mutate(match = ifelse(true_value == predicted_value, TRUE, FALSE))

  accuracy_knn <- compare_knn %>%
    group_by(match) %>%
    summarise(n = sum(n)) %>%
    ungroup() %>%
    mutate(percent = n/sum(n),
           k = i,
           model = 'knn') %>%
    filter(match == TRUE)

  select_k <- rbind(select_k, accuracy_knn)

}

# 1 is the best
kresult <- class::knn(train = train_set[,-1], test = test_set[,-1], cl = train_resp, k = 1)

# check
@@ -205,9 +235,17 @@ compare_knn %>%
### Random Forest

``` r
rfresult <- randomForest(as.factor(playlist_genre) ~ ., ntree = 100, data = train_set)

varImpPlot(rfresult, sort = TRUE, n.var = 11)
rfresult <- randomForest(as.factor(playlist_genre) ~ ., ntree = 100, importance = TRUE, data = train_set)

importance(rfresult, type = 1) %>%
  as.data.frame() %>%
  mutate(measure = row.names(.)) %>%
  ggplot(aes(x = reorder(measure, MeanDecreaseAccuracy), y = MeanDecreaseAccuracy)) +
  geom_point() +
  coord_flip() +
  theme_kp() +
  labs(title = 'Random Forest Variable Importance',
       y = 'Mean Decrease in Accuracy', x = '')
```

![](spotify_genres_files/figure-gfm/random_forest-1.png)<!-- -->
@@ -252,5 +290,11 @@ accuracy_knn %>%

| model         | accuracy |
| :------------ | -------: |
| knn           |   34.87% |
| random forest |   57.61% |
| knn           |   30.61% |
| random forest |   53.53% |

## Subgenres

Can we predict subgenres within a parent genre? How accurate would we be
if we predicted the parent genre, then moved onto a model to predict the
sub?
